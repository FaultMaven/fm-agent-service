# -----------------------------------------------------------------------------
# FaultMaven Agent Service Configuration
# -----------------------------------------------------------------------------
# NOTE: This file uses variable interpolation (${VAR}) which works in Docker
# and Docker Compose environments. For local development without Docker,
# manually expand these values or use Docker Compose to load this file.
# -----------------------------------------------------------------------------

# Service Settings
ENVIRONMENT=development
PORT=8007
LOG_LEVEL=INFO

# Service Mesh URLs
FM_KNOWLEDGE_SERVICE_URL=http://localhost:8006
FM_CASE_SERVICE_URL=http://localhost:8003
FM_INVESTIGATION_SERVICE_URL=http://localhost:8005

# -----------------------------------------------------------------------------
# Global LLM Defaults (Single Source of Truth)
# -----------------------------------------------------------------------------
# Change these values here to update them everywhere below.
# This provides centralized configuration for provider selection and model
# preferences across all task types (chat, multimodal, synthesis).
# -----------------------------------------------------------------------------
PRIMARY_LLM_PROVIDER=openai          # Main provider: openai, anthropic, groq, local, etc.
DEFAULT_FAST_MODEL=gpt-4o-mini       # Cost-effective model for high-volume tasks
DEFAULT_SMART_MODEL=gpt-4o           # Premium model for complex reasoning
DEFAULT_LOCAL_URL=http://localhost:11434/v1  # Ollama default endpoint

# -----------------------------------------------------------------------------
# Provider Configuration
# -----------------------------------------------------------------------------
# Configure ONE or more providers. The service tries providers in the order
# they're defined, providing automatic fallback if one fails.
# Add more providers for redundancy (recommended for production).
# -----------------------------------------------------------------------------

# 1. OpenAI
OPENAI_API_KEY=                         # Get from: https://platform.openai.com/api-keys
OPENAI_MODEL=${DEFAULT_FAST_MODEL}      # Expands to: gpt-4o-mini

# 2. Anthropic
ANTHROPIC_API_KEY=                      # Get from: https://console.anthropic.com/
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# 3. Groq (FREE tier available!)
GROQ_API_KEY=                           # Get from: https://console.groq.com/
GROQ_MODEL=llama-3.3-70b-versatile

# 4. Gemini (Google)
GEMINI_API_KEY=                         # Get from: https://makersuite.google.com/app/apikey
GEMINI_MODEL=gemini-1.5-pro

# 5. Fireworks AI
FIREWORKS_API_KEY=                      # Get from: https://fireworks.ai/api-keys
FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct

# 6. HuggingFace Inference API
# ⚠️ WARNING: Free tier has 10-60 second cold starts! Consider Groq for FREE fast inference.
HUGGINGFACE_API_KEY=                    # Get from: https://huggingface.co/settings/tokens
HUGGINGFACE_MODEL=meta-llama/Llama-3.2-3B-Instruct

# 7. OpenRouter (aggregates multiple providers)
OPENROUTER_API_KEY=                     # Get from: https://openrouter.ai/keys
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# 8. Local LLM (Ollama, LM Studio) - FREE, runs locally!
# Set API Key to "lm-studio" if no auth is required
LOCAL_LLM_API_KEY=lm-studio             # Use "lm-studio" for no-auth servers
LOCAL_LLM_BASE_URL=${DEFAULT_LOCAL_URL} # Expands to: http://localhost:11434/v1
LOCAL_LLM_MODEL=llama3                  # Model loaded in Ollama/LM Studio

# -----------------------------------------------------------------------------
# Task Routing (Aliases)
# -----------------------------------------------------------------------------
# Maps specific task types to providers and models for cost optimization.
# By default, all tasks use PRIMARY_LLM_PROVIDER defined above.
# Override individual tasks for advanced cost/performance tuning.
# -----------------------------------------------------------------------------

# Main Chat: Interactive diagnostic conversations
CHAT_PROVIDER=${PRIMARY_LLM_PROVIDER}   # Expands to: openai
CHAT_MODEL=${DEFAULT_SMART_MODEL}       # Expands to: gpt-4o

# Multimodal: Visual evidence processing (images, screenshots) [future feature]
MULTIMODAL_PROVIDER=${PRIMARY_LLM_PROVIDER}
MULTIMODAL_MODEL=${DEFAULT_SMART_MODEL}

# Synthesis: Knowledge base RAG queries (high volume) [future feature]
# Uses fast/cheap model for cost savings on high-frequency operations
SYNTHESIS_PROVIDER=${PRIMARY_LLM_PROVIDER}
SYNTHESIS_MODEL=${DEFAULT_FAST_MODEL}   # Expands to: gpt-4o-mini

# Strict Mode: If true, fails when assigned provider is unavailable (no fallback)
# Set to false for automatic fallback to other configured providers
STRICT_PROVIDER_MODE=false

# -----------------------------------------------------------------------------
# Opik LLM Observability
# -----------------------------------------------------------------------------
OPIK_URL=http://localhost:8080
OPIK_WORKSPACE=faultmaven
OPIK_PROJECT=FaultMaven
OPIK_API_KEY=
OPIK_TRACK_DISABLE=false

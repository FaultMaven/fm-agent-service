# FaultMaven Agent Service - Environment Configuration

# ============================================================================
# LLM Provider Configuration (at least one required)
# ============================================================================
# FaultMaven supports 7 LLM providers with automatic fallback.
# The simplest setup: configure ONE provider and it works for all tasks.
#
# Supported Providers:
# - Cloud LLMs: OpenAI, Anthropic, Groq, Gemini, Fireworks, OpenRouter
# - Local LLMs: Ollama, LM Studio, LocalAI, vLLM (free, private, runs on your machine)
#
# Get API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
# - Groq: https://console.groq.com/ (FREE tier available - ultra-fast!)
# - Gemini: https://makersuite.google.com/app/apikey
# - Fireworks: https://fireworks.ai/api-keys
# - OpenRouter: https://openrouter.ai/keys (aggregates multiple providers)

# ============================================================================
# BASIC CONFIGURATION - One LLM for All Tasks (Recommended)
# ============================================================================
# Choose ONE option below. This LLM will handle all AI tasks in FaultMaven.

# Option 1: Cloud LLM (OpenAI, Anthropic, etc.)
OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GROQ_API_KEY=gsk_...
# GEMINI_API_KEY=AIza...
# FIREWORKS_API_KEY=fw_...
# OPENROUTER_API_KEY=sk-or-...

# Option 2: Local LLM (FREE, private, runs locally)
# LOCAL_LLM_API_KEY=not-needed
# LOCAL_LLM_URL=http://localhost:11434/v1
# LOCAL_LLM_MODEL=llama3.1
#
# Setup Instructions:
# 1. Install Ollama: https://ollama.ai/download
# 2. Run: ollama serve
# 3. Pull a model: ollama pull llama3.1
# 4. Uncomment the 3 LOCAL_LLM variables above
# 5. Comment out any cloud provider API keys
#
# Other local LLM servers (use same variables):
# - LM Studio: http://localhost:1234/v1
# - LocalAI: http://localhost:8080/v1
# - vLLM: http://localhost:8000/v1

# ============================================================================
# OPTIONAL: Multiple Providers (Fallback and Redundancy)
# ============================================================================
# Configure multiple providers for automatic fallback if one fails.
# Just add additional API keys - FaultMaven will try them in order.
#
# Example: Primary OpenAI, fallback to Groq
# OPENAI_API_KEY=sk-...
# GROQ_API_KEY=gsk_...

# ============================================================================
# ADVANCED: Model Override (Optional)
# ============================================================================
# Override the default model for any provider.
# Useful for testing newer models or cost optimization.
#
# OPENAI_MODEL=gpt-4o-mini
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# GROQ_MODEL=llama-3.3-70b-versatile
# GEMINI_MODEL=gemini-1.5-pro
# FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
# LOCAL_LLM_MODEL=llama3.1

# ============================================================================
# ADVANCED: Base URL Override (Optional)
# ============================================================================
# Only needed for Azure OpenAI, custom proxies, or alternative endpoints.
#
# OPENAI_BASE_URL=https://api.openai.com/v1
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
# GROQ_BASE_URL=https://api.groq.com/openai/v1
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# FIREWORKS_BASE_URL=https://api.fireworks.ai/inference/v1
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# LOCAL_LLM_URL=http://localhost:11434/v1

# ============================================================================
# ADVANCED: Task-Specific Provider Routing (Optional)
# ============================================================================
# For cost optimization, assign specific providers to different task types.
# By default, FaultMaven uses the same provider for all tasks (simplest setup).
#
# Task Types:
#   - CHAT: Main diagnostic conversations (most frequent)
#   - MULTIMODAL: Visual evidence processing (images, screenshots) [future feature]
#   - SYNTHESIS: Knowledge base RAG queries (high volume) [future feature]
#
# Example 1: Same provider, different models for cost optimization
# CHAT_PROVIDER=openai
# CHAT_MODEL=gpt-4o                    # Premium model for chat
#
# SYNTHESIS_PROVIDER=openai
# SYNTHESIS_MODEL=gpt-4o-mini          # Cheaper model for RAG (10x savings!)
#
# Example 2: Mix cloud and local LLMs
# CHAT_PROVIDER=openai
# CHAT_MODEL=gpt-4o                    # Cloud LLM for complex chat
#
# SYNTHESIS_PROVIDER=local
# SYNTHESIS_MODEL=llama3.1             # Local LLM for RAG (free!)
#
# Example 3: Different providers for different tasks
# CHAT_PROVIDER=anthropic
# CHAT_MODEL=claude-3-5-sonnet-20241022
#
# MULTIMODAL_PROVIDER=gemini
# MULTIMODAL_MODEL=gemini-1.5-pro      # Excellent vision capabilities
#
# SYNTHESIS_PROVIDER=groq
# SYNTHESIS_MODEL=llama-3.1-8b-instant # FREE tier, ultra-fast
#
# STRICT_PROVIDER_MODE=false           # If true, fails instead of falling back

# ============================================================================
# Service Communication
# ============================================================================
CASE_SERVICE_URL=http://fm-case-service:8003

# ============================================================================
# Storage Configuration (Pluggable Backends)
# ============================================================================
# Case storage: inmemory (dev/testing) | postgres (production)
CASE_STORAGE_TYPE=inmemory

# Session storage: inmemory (dev/testing) | redis (production)
SESSION_STORAGE_TYPE=redis

# Vector storage: inmemory (dev/testing) | chromadb (production)
VECTOR_STORAGE_TYPE=chromadb

# ============================================================================
# Opik LLM Observability (Internal ClusterIP)
# ============================================================================
OPIK_URL=http://opik-backend.opik-system.svc.cluster.local:8080
OPIK_WORKSPACE=faultmaven
OPIK_PROJECT=FaultMaven
OPIK_API_KEY=
OPIK_TRACK_DISABLE=false

# ============================================================================
# Application Settings
# ============================================================================
PORT=8000
LOG_LEVEL=INFO
ENABLE_TRACING=true
